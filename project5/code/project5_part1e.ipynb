{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import DataFrame\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import matplotlib.pyplot as plt\n",
    "import statsmodels.api as sm\n",
    "from sklearn.cross_validation import cross_val_predict\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "import sys\n",
    "import warnings\n",
    "\n",
    "if not sys.warnoptions:\n",
    "    warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "traingDict = {'#GoHawks' : ['tweets_#gohawks.txt', 188136],\n",
    "                       '#GoPatriots' : ['tweets_#gopatriots.txt', 26232],\n",
    "                       '#NFL' : ['tweets_#nfl.txt', 259024],\n",
    "                       '#Patriots' : ['tweets_#patriots.txt', 489713],\n",
    "                       '#SB49' : ['tweets_#sb49.txt', 826951],\n",
    "                       '#SuperBowl' : ['tweets_#superbowl.txt', 1348767]}\n",
    "    \n",
    "testingDict = {1 : ['sample1_period1.txt', 730],\n",
    "                      2 : ['sample2_period2.txt', 212273],\n",
    "                      3 : ['sample3_period3.txt', 3628],\n",
    "                      4 : ['sample4_period1.txt', 1646],\n",
    "                      5 : ['sample5_period1.txt', 2059],\n",
    "                      6 : ['sample6_period2.txt', 205554],\n",
    "                      7 : ['sample7_period3.txt', 528],\n",
    "                      8 : ['sample8_period1.txt', 229],\n",
    "                      9 : ['sample9_period2.txt', 11311],\n",
    "                      10 : ['sample10_period3.txt', 365]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def fetch_data(filename_key, dataDict, testingData):                         \n",
    "                    \n",
    "   \n",
    "    timeStamps = [0]*dataDict[filename_key][1]\n",
    "    isRetweet = [False]*dataDict[filename_key][1]\n",
    "    userFollowers = [0]*dataDict[filename_key][1]\n",
    "    \n",
    "    numURLCitations = [0]*dataDict[filename_key][1]\n",
    "    authorNames = ['']*dataDict[filename_key][1]\n",
    "    numMentions = [0]*dataDict[filename_key][1]\n",
    "    rankingScores = [0.0]*dataDict[filename_key][1]\n",
    "    numHashtags = [0]*dataDict[filename_key][1]\n",
    "    \n",
    "    filePath = ''\n",
    "    postingTimeFeature = ''\n",
    "    if testingData:\n",
    "        filePath = './test_data/'+dataDict[filename_key][0]\n",
    "        postingTimeFeature = 'firstpost_date'\n",
    "    else:\n",
    "        filePath = 'tweet_data/'+dataDict[filename_key][0]\n",
    "        postingTimeFeature = 'citation_date'\n",
    "        \n",
    "\n",
    "    input_file = open(filePath)\n",
    "    for (line, index) in zip(input_file, range(0, dataDict[filename_key][1])):\n",
    "        data = json.loads(line)\n",
    "        timeStamps[index] = data[postingTimeFeature]\n",
    "        userFollowers[index] = data['author']['followers']\n",
    "\n",
    "        authorName = data['author']['nick']\n",
    "        originalAuthor = data['original_author']['nick']\n",
    "        if authorName != originalAuthor:\n",
    "            isRetweet[index] = True\n",
    "\n",
    "\n",
    "        numURLCitations[index] = len(data['tweet']['entities']['urls'])\n",
    "        authorNames[index] = authorName\n",
    "        numMentions[index] = len(data['tweet']['entities']['user_mentions'])\n",
    "        rankingScores[index] = data['metrics']['ranking_score']\n",
    "        numHashtags[index] = data['title'].count('#')\n",
    "        \n",
    "        \n",
    "    input_file.close()\n",
    "    \n",
    "\n",
    "    startTime = 1421222400\n",
    "    if testingData:\n",
    "        startTime = (min(timeStamps)/3600)*3600\n",
    "\n",
    "    hours_passed = int((max(timeStamps)-startTime)/3600)+1\n",
    "    hourlyTweets = [0] * hours_passed\n",
    "    hourlyRetweets = [0] * hours_passed\n",
    "    hourlyFollowerSum = [0] * hours_passed\n",
    "    hourlyMaxFollowers = [0] * hours_passed\n",
    "    hourlyTime = [0] * hours_passed\n",
    "\n",
    "    \n",
    "    hourlyURLCitations = [0] * hours_passed\n",
    "    hourlyAuthors = [0] * hours_passed\n",
    "    hourlyAuthorSet = [0] * hours_passed\n",
    "    for i in range(0, hours_passed):\n",
    "        hourlyAuthorSet[i] = set([])\n",
    "    hourlyMentions = [0] * hours_passed\n",
    "    hourlyRankingScores = [0.0] * hours_passed\n",
    "    hourlyHashtags = [0] * hours_passed\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(0, dataDict[filename_key][1]):\n",
    "        current_hour = int((timeStamps[i]-startTime)/3600)\n",
    "        \n",
    "        hourlyTweets[current_hour] += 1\n",
    "        if isRetweet[i]:\n",
    "            hourlyRetweets[current_hour] += 1\n",
    "                                      \n",
    "        hourlyFollowerSum[current_hour] += userFollowers[i]\n",
    "    \n",
    "        if userFollowers[i] > hourlyMaxFollowers[current_hour]:\n",
    "            hourlyMaxFollowers[current_hour] = userFollowers[i]\n",
    "\n",
    "\n",
    "        hourlyURLCitations[current_hour] += numURLCitations[i]\n",
    "        hourlyAuthorSet[current_hour].add(authorNames[i])\n",
    "        hourlyMentions[current_hour] += numMentions[i]\n",
    "        hourlyRankingScores[current_hour] += rankingScores[i]\n",
    "        hourlyHashtags[current_hour] += numHashtags[i]\n",
    "\n",
    "\n",
    "    for i in range(0, len(hourlyAuthorSet)):\n",
    "        hourlyAuthors[i] = len(hourlyAuthorSet[i])\n",
    "    \n",
    "    if testingData:\n",
    "        for i in range(0, len(hourlyTime)):\n",
    "            hourlyTime[i] = ((startTime-1421222400)/3600+i)%24\n",
    "    else:\n",
    "        for i in range(0, len(hourlyTime)):\n",
    "            hourlyTime[i] = i%24\n",
    "\n",
    "    \n",
    "    target_value = hourlyTweets[1:]\n",
    "    target_value.append(0)\n",
    "    data = np.array([hourlyTweets,\n",
    "                     hourlyRetweets,\n",
    "                     hourlyFollowerSum,\n",
    "                     hourlyMaxFollowers,\n",
    "                     hourlyTime,\n",
    "                     hourlyURLCitations,\n",
    "                     hourlyAuthors,\n",
    "                     hourlyMentions,\n",
    "                     hourlyRankingScores,\n",
    "                     hourlyHashtags,\n",
    "                     target_value])\n",
    "    data = np.transpose(data)\n",
    "    df = DataFrame(data)\n",
    "    df.columns = ['num_tweets', \n",
    "                  'num_retweets', \n",
    "                  'sum_followers',\n",
    "                  'max_followers',\n",
    "                  'timeDay',\n",
    "                  'num_URLs',\n",
    "                  'num_authors',\n",
    "                  'num_mensions',\n",
    "                  'ranking_score',\n",
    "                  'num_hashtags',\n",
    "                  'target_value']\n",
    "    if os.path.isdir('./retrievedData'):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir('./retrievedData')\n",
    "        \n",
    "    \n",
    "    if testingData:\n",
    "        df.to_csv('./retrievedData/Q5_'+dataDict[filename_key][0][:-4]+'.csv', index = False)  \n",
    "    else:\n",
    "        df.to_csv('./retrievedData/Q5_'+filename_key+'.csv', index = False)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch_data(filename_key, dataDict, testingData):                         \n",
    "                    \n",
    "   \n",
    "    timeStamps = [0]*dataDict[filename_key][1]\n",
    "    isRetweet = [False]*dataDict[filename_key][1]\n",
    "    userFollowers = [0]*dataDict[filename_key][1]\n",
    "    \n",
    "    numURLCitations = [0]*dataDict[filename_key][1]\n",
    "    authorNames = ['']*dataDict[filename_key][1]\n",
    "    numMentions = [0]*dataDict[filename_key][1]\n",
    "    rankingScores = [0.0]*dataDict[filename_key][1]\n",
    "    numHashtags = [0]*dataDict[filename_key][1]\n",
    "    \n",
    "    filePath = ''\n",
    "    postingTimeFeature = ''\n",
    "    if testingData:\n",
    "        filePath = './test_data/'+dataDict[filename_key][0]\n",
    "        postingTimeFeature = 'firstpost_date'\n",
    "    else:\n",
    "        filePath = 'tweet_data/'+dataDict[filename_key][0]\n",
    "        postingTimeFeature = 'citation_date'\n",
    "        \n",
    "\n",
    "    input_file = open(filePath)\n",
    "    for (line, index) in zip(input_file, range(0, dataDict[filename_key][1])):\n",
    "        data = json.loads(line)\n",
    "        timeStamps[index] = data[postingTimeFeature]\n",
    "        userFollowers[index] = data['author']['followers']\n",
    "\n",
    "        authorName = data['author']['nick']\n",
    "        originalAuthor = data['original_author']['nick']\n",
    "        if authorName != originalAuthor:\n",
    "            isRetweet[index] = True\n",
    "\n",
    "\n",
    "        numURLCitations[index] = len(data['tweet']['entities']['urls'])\n",
    "        authorNames[index] = authorName\n",
    "        numMentions[index] = len(data['tweet']['entities']['user_mentions'])\n",
    "        rankingScores[index] = data['metrics']['ranking_score']\n",
    "        numHashtags[index] = data['title'].count('#')\n",
    "        \n",
    "        \n",
    "    input_file.close()\n",
    "    \n",
    "\n",
    "    startTime = 1421222400\n",
    "    if testingData:\n",
    "        startTime = (min(timeStamps)/3600)*3600\n",
    "\n",
    "    hours_passed = int((max(timeStamps)-startTime)/3600)+1\n",
    "    hourlyTweets = [0] * hours_passed\n",
    "    hourlyRetweets = [0] * hours_passed\n",
    "    hourlyFollowerSum = [0] * hours_passed\n",
    "    hourlyMaxFollowers = [0] * hours_passed\n",
    "    hourlyTime = [0] * hours_passed\n",
    "\n",
    "    \n",
    "    hourlyURLCitations = [0] * hours_passed\n",
    "    hourlyAuthors = [0] * hours_passed\n",
    "    hourlyAuthorSet = [0] * hours_passed\n",
    "    for i in range(0, hours_passed):\n",
    "        hourlyAuthorSet[i] = set([])\n",
    "    hourlyMentions = [0] * hours_passed\n",
    "    hourlyRankingScores = [0.0] * hours_passed\n",
    "    hourlyHashtags = [0] * hours_passed\n",
    "    \n",
    "    \n",
    "    \n",
    "    for i in range(0, dataDict[filename_key][1]):\n",
    "        current_hour = int((timeStamps[i]-startTime)/3600)\n",
    "        \n",
    "        hourlyTweets[current_hour] += 1\n",
    "        if isRetweet[i]:\n",
    "            hourlyRetweets[current_hour] += 1\n",
    "                                      \n",
    "        hourlyFollowerSum[current_hour] += userFollowers[i]\n",
    "    \n",
    "        if userFollowers[i] > hourlyMaxFollowers[current_hour]:\n",
    "            hourlyMaxFollowers[current_hour] = userFollowers[i]\n",
    "\n",
    "\n",
    "        hourlyURLCitations[current_hour] += numURLCitations[i]\n",
    "        hourlyAuthorSet[current_hour].add(authorNames[i])\n",
    "        hourlyMentions[current_hour] += numMentions[i]\n",
    "        hourlyRankingScores[current_hour] += rankingScores[i]\n",
    "        hourlyHashtags[current_hour] += numHashtags[i]\n",
    "\n",
    "\n",
    "    for i in range(0, len(hourlyAuthorSet)):\n",
    "        hourlyAuthors[i] = len(hourlyAuthorSet[i])\n",
    "    \n",
    "    if testingData:\n",
    "        for i in range(0, len(hourlyTime)):\n",
    "            hourlyTime[i] = ((startTime-1421222400)/3600+i)%24\n",
    "    else:\n",
    "        for i in range(0, len(hourlyTime)):\n",
    "            hourlyTime[i] = i%24\n",
    "\n",
    "    \n",
    "    target_value = hourlyTweets[1:]\n",
    "    target_value.append(0)\n",
    "    data = np.array([hourlyTweets,\n",
    "                     hourlyRetweets,\n",
    "                     hourlyFollowerSum,\n",
    "                     hourlyMaxFollowers,\n",
    "                     hourlyTime,\n",
    "                     hourlyURLCitations,\n",
    "                     hourlyAuthors,\n",
    "                     hourlyMentions,\n",
    "                     hourlyRankingScores,\n",
    "                     hourlyHashtags,\n",
    "                     target_value])\n",
    "    data = np.transpose(data)\n",
    "    df = DataFrame(data)\n",
    "    df.columns = ['num_tweets', \n",
    "                  'num_retweets', \n",
    "                  'sum_followers',\n",
    "                  'max_followers',\n",
    "                  'timeDay',\n",
    "                  'num_URLs',\n",
    "                  'num_authors',\n",
    "                  'num_mensions',\n",
    "                  'ranking_score',\n",
    "                  'num_hashtags',\n",
    "                  'target_value']\n",
    "    if os.path.isdir('./retrievedData'):\n",
    "        pass\n",
    "    else:\n",
    "        os.mkdir('./retrievedData')\n",
    "        \n",
    "    \n",
    "    if testingData:\n",
    "        df.to_csv('./retrievedData/Q5_'+dataDict[filename_key][0][:-4]+'.csv', index = False)  \n",
    "    else:\n",
    "        df.to_csv('./retrievedData/Q5_'+filename_key+'.csv', index = False)  \n",
    "\n",
    "\n",
    "def oneHotEncoder(df):\n",
    "    timeDaySet = range(0,24)\n",
    "    for timeDay in timeDaySet:\n",
    "        timeDayColumn = []\n",
    "        for timeDay_item in df['timeDay']:\n",
    "            if timeDay_item == timeDay:\n",
    "                timeDayColumn.append(1)\n",
    "            else:\n",
    "                timeDayColumn.append(0)\n",
    "        df.insert(df.shape[1]-1,\n",
    "                  str(timeDay)+'th_hour',\n",
    "                  timeDayColumn)\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "def data_regression(training_hashtag, testing_data_index):\n",
    "    training_x = pd.read_csv('./retrievedData/Q5_'+training_hashtag+'.csv')\n",
    "    testing_x = pd.read_csv('./retrievedData/Q5_'+testingDict[testing_data_index][0][:-4]+'.csv')\n",
    "    \n",
    "    training_x = oneHotEncoder(training_x)\n",
    "    testing_x = oneHotEncoder(testing_x)\n",
    "    \n",
    "\n",
    " \n",
    "    training_x.drop('timeDay', 1, inplace = True)\n",
    "    training_y = training_x.pop('target_value')\n",
    "    \n",
    "    testing_x.drop('timeDay', 1, inplace = True)\n",
    "    testing_y = testing_x.pop('target_value')\n",
    "    \n",
    "    \n",
    "    training_x_before_event = training_x[:440]\n",
    "    training_x_during_event = training_x[440:452]\n",
    "    training_x_after_event = training_x[452:]\n",
    "        \n",
    "    training_y_before_event = training_y[:440]\n",
    "    training_y_during_event = training_y[440:452]\n",
    "    training_y_after_event = training_y[452:]\n",
    "    \n",
    "        \n",
    "\n",
    "    \n",
    "    regressor_before_event = RandomForestRegressor(n_estimators = 20, max_depth = 9)\n",
    "    regressor_during_event = RandomForestRegressor(n_estimators = 20, max_depth = 9)\n",
    "    regressor_after_event = RandomForestRegressor(n_estimators = 20, max_depth = 9)\n",
    "\n",
    "    regressor_before_event.fit(training_x_before_event,training_y_before_event)\n",
    "    regressor_during_event.fit(training_x_during_event,training_y_during_event)\n",
    "    regressor_after_event.fit(training_x_after_event,training_y_after_event)\n",
    "    \n",
    "    \n",
    "    \n",
    "    predicted_y = []\n",
    "    if testingDict[testing_data_index][0][-5] == '1':\n",
    "        predicted_y = regressor_before_event.predict(testing_x)\n",
    "    elif testingDict[testing_data_index][0][-5] == '2':\n",
    "        predicted_y = regressor_during_event.predict(testing_x)\n",
    "    else:\n",
    "        predicted_y = regressor_after_event.predict(testing_x)\n",
    "    \n",
    "    \n",
    "  \n",
    "    data = np.array([predicted_y, testing_y])\n",
    "    data = np.transpose(data)\n",
    "    results = DataFrame(data)\n",
    "    results.columns = ['Predicted', 'Actual']\n",
    "    #print results\n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "    total_error = 0.0\n",
    "    for i in range(len(testing_y)-1):\n",
    "        total_error += abs(testing_y[i] - predicted_y[i])\n",
    "    #print 'Average prediction error:',total_error/len(testing_y)\n",
    "    \n",
    "    return results, total_error/(len(testing_y)-1)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predict(testing_data_index):\n",
    "    \n",
    "    result_list = []\n",
    "    error_list = []\n",
    "\n",
    "    hashtags = ['#GoHawks',\n",
    "                    '#GoPatriots',\n",
    "                    '#NFL',\n",
    "                    '#Patriots',\n",
    "                    '#SB49',\n",
    "                    '#SuperBowl']\n",
    "    for hashtag in hashtags:\n",
    "        result, error = data_regression(hashtag, testing_data_index)\n",
    "        result_list.append(result)\n",
    "        error_list.append(error)\n",
    "    minimum_index = 0\n",
    "    minimum_error = error_list[0]\n",
    "    for i in range(0, len(error_list)):\n",
    "        if error_list[i] < minimum_error:\n",
    "            minimum_error = error_list[i]\n",
    "            minimum_index = i\n",
    "\n",
    "    return result_list[minimum_index],minimum_error,hashtags[minimum_index]\n",
    "\n",
    "\n",
    "\n",
    "def fetch_results(testing_data_index):\n",
    "    bestResult, minimum_error, optimal_hashtag = predict(testing_data_index)\n",
    "    \n",
    "    for i in range(20):\n",
    "        result, error, hashtag = predict(testing_data_index)\n",
    "        if error  < minimum_error:\n",
    "            minimum_error = error\n",
    "            bestResult = result\n",
    "            optimal_hashtag = hashtag\n",
    "    \n",
    "            \n",
    "    print '-'*20\n",
    "    print bestResult\n",
    "    print '-'*20\n",
    "\n",
    "\n",
    "\n",
    "def part5(): \n",
    "    fetch_data('#GoHawks',traingDict,False)\n",
    "    fetch_data('#GoPatriots',traingDict,False)\n",
    "    fetch_data('#NFL',traingDict,False)\n",
    "    fetch_data('#Patriots',traingDict,False)\n",
    "    fetch_data('#SB49',traingDict,False)\n",
    "    fetch_data('#SuperBowl',traingDict,False)\n",
    "\n",
    "    for i in range(1,11):\n",
    "        fetch_data(i,testingDict,True)\n",
    "\n",
    "    for i in range(1,11):\n",
    "        fetch_results(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "    Predicted  Actual\n",
      "0  119.122440    82.0\n",
      "1   77.068261    68.0\n",
      "2   93.081802    94.0\n",
      "3   94.100586   171.0\n",
      "4  158.796791   178.0\n",
      "5  207.198387     0.0\n",
      "--------------------\n",
      "--------------------\n",
      "   Predicted   Actual\n",
      "0   20354.10   9361.0\n",
      "1   19674.25  10374.0\n",
      "2   25444.55  20066.0\n",
      "3   83508.45  81958.0\n",
      "4   89970.10  82923.0\n",
      "5  116255.75      0.0\n",
      "--------------------\n",
      "--------------------\n",
      "    Predicted  Actual\n",
      "0  474.263333   550.0\n",
      "1  508.108333   610.0\n",
      "2  571.908333   888.0\n",
      "3  650.575000   616.0\n",
      "4  521.813333   523.0\n",
      "5  468.463333     0.0\n",
      "--------------------\n",
      "--------------------\n",
      "    Predicted  Actual\n",
      "0  322.781043   257.0\n",
      "1  230.964448   236.0\n",
      "2  267.207538   266.0\n",
      "3  243.025395   267.0\n",
      "4  253.053030   201.0\n",
      "5  197.633562     0.0\n",
      "--------------------\n",
      "--------------------\n",
      "    Predicted  Actual\n",
      "0  408.951316   508.0\n",
      "1  426.351316   353.0\n",
      "2  398.851316   362.0\n",
      "3  408.951316   281.0\n",
      "4  224.101316   213.0\n",
      "5  103.101316     0.0\n",
      "--------------------\n",
      "--------------------\n",
      "   Predicted   Actual\n",
      "0   13729.05  12931.0\n",
      "1   43026.70  60619.0\n",
      "2   28606.55  52699.0\n",
      "3   30442.05  41019.0\n",
      "4   28061.75  37307.0\n",
      "5   29696.20      0.0\n",
      "--------------------\n",
      "--------------------\n",
      "    Predicted  Actual\n",
      "0  101.199881   102.0\n",
      "1   85.990537    66.0\n",
      "2   58.183742    60.0\n",
      "3   54.390919    55.0\n",
      "4   49.365499   120.0\n",
      "5   97.848150     0.0\n",
      "--------------------\n",
      "--------------------\n",
      "   Predicted  Actual\n",
      "0  45.287279    72.0\n",
      "1  56.448098    56.0\n",
      "2  39.996021    41.0\n",
      "3  23.586515    11.0\n",
      "4  36.899291     0.0\n",
      "--------------------\n",
      "--------------------\n",
      "   Predicted  Actual\n",
      "0    1723.75  1734.0\n",
      "1    1622.15  1619.0\n",
      "2    1613.65  1582.0\n",
      "3    1906.95  1857.0\n",
      "4    2050.85  2790.0\n",
      "5    2007.20     0.0\n",
      "--------------------\n",
      "--------------------\n",
      "   Predicted  Actual\n",
      "0  59.420245    54.0\n",
      "1  55.720447    68.0\n",
      "2  63.349655    62.0\n",
      "3  51.817388    58.0\n",
      "4  49.702804    61.0\n",
      "5  68.202083     0.0\n",
      "--------------------\n"
     ]
    }
   ],
   "source": [
    "part5()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
